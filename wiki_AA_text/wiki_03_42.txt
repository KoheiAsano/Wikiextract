検索エンジン
検索エンジン（けんさくエンジン、）は、狭義にはインターネットに存在する情報（ウェブページ、ウェブサイト、画像ファイル、ネットニュースなど）を検索する機能およびそのプログラム。インターネットの普及初期には、検索としての機能のみを提供していたウェブサイトそのものを検索エンジンと呼んだが、現在では様々なサービスが加わったポータルサイト化が進んだため、検索をサービスの一つとして提供するウェブサイトを単に検索サイトと呼ぶことはなくなっている。広義には、インターネットに限定せず情報を検索するシステム全般を含む。
狭義の検索エンジンは、ロボット型検索エンジン、ディレクトリ型検索エンジン、メタ検索エンジンなどに分類される。広義の検索エンジンとしては、ある特定のウェブサイト内に登録されているテキスト情報の全文検索機能を備えたソフトウェア（全文検索システム）等がある。
検索エンジンは、検索窓と呼ばれるボックスにキーワードを入力して検索をかけるもので、全文検索が可能なものと不可能なものとがある。検索サイトを一般に「検索エンジン」と呼ぶことはあるが、厳密には検索サイト自体は検索エンジンでない。
与えられた検索式に従って、ウェブページ等を検索するサーバ、システムのこと。検索式は、最も単純な場合はキーワードとなる文字列のみであるが、複数のキーワードにAND（「かつ」、論理積）やOR（「または」、論理和）等の論理条件を組み合わせて指定することができるものが多い。
ロボット型検索エンジンの大きな特徴の一つとして、クローラ（ロボット・スパイダー）を用いることが挙げられる。このことにより、WWW上にある多数の情報を効率よく収集（日本の著作権法では複製）することができる。大規模な検索エンジンでは、80億ページ以上のページから検索が可能になっている。
収集したページの情報は、前もって解析し、索引情報（インデックス）を作成する（日本の著作権法では編集）。日本語などの言語では、自然言語処理機能が生成される索引の質に影響する。このため、多言語対応した検索エンジンの方が精度の高い検索が可能となる。
検索結果の表示順は、検索エンジンの質が最も問われる部分である。ユーザーが期待したページを検索結果の上位に表示することができなければ、ユーザーが離れてしまうからである。そのため、多くの検索エンジンが、表示順を決定するアルゴリズムを非公開にし、その性能を競っている。検索エンジン最適化業者の存在も、アルゴリズムを公開しない要因になっている。Googleは、そのアルゴリズムの一部であるPageRankを公開しているが、やはり、多くの部分が非公開になっている。Googleの場合、創設初期におけるアルゴリズムについては、創設者自身がウェブ上で公表している論文でその一端を知ることができる。
参照 英語原文日本語の解説
ウェブページの更新時刻の情報を用いて、新しい情報に限定して検索できるものや、検索結果をカテゴリ化して表示するものなど、特長のある機能を搭載したり、検索結果をユーザーへ最適化していく動きもある。
従来のウェブページを検索するだけの検索エンジンにとどまらず、最近ではインターネットショッピング専用の検索エンジンなど、特定の分野に特化した検索エンジンの開発も散見される。商品検索では、価格比較サービス日本最大手の価格.comや、ベンチャー企業が開発するQOOPIEなどある。また、職業検索エンジンとしてはCraigslistなどがある。
Google、Yahoo!、インフォシーク、テクノラティ、MARSFLAG、Altavista、ムーター、AlltheWeb、Teoma、WiseNut、Inktomi、SAGOOL、Yahoo! JAPAN (2005.10〜) など。
人手で構築したウェブディレクトリ内を検索するサーバ、システムのこと。
人手で構築しているため、質の高いウェブサイトを検索可能。概要を人手で記入しているため、検索結果の一覧から目的のサイトを探しやすい、サイトのカテゴリ分けがされていることから、特定分野や地区などに限定したサイトを探しやすいという特長がある。
しかし、検索対象となるサイトは人手で入力するため、検索対象となるサイト数が多くできないという欠点がある。
インターネットが一般に使われるようになった初期（1990年代）のころには、ディレクトリ型が主体であったが、WWWの爆発的な拡大によって、あらゆるウェブサイトを即時にディレクトリに反映させることが事実上不可能になり、現在では主流ではなくなっている。
このため、ディレクトリ型検索エンジンでは、検索にヒットするサイトが無かった場合、ロボット型検索エンジンを用いて結果を表示するような、併用型のものが多い。
日立国際ビジネスのHole-in-One（〜04.11）、Yahoo! JAPAN（〜05.10）、LookSmart Japan（〜06.05）、goo、infoseek、Open Directory Projectなど。
P2P通信によってウェブコンテンツのインデックスを多数のピアに分散させ、P2Pネットワーク全体で各ピアの持つインデックスを共有する検索システムのこと。
ウェブのクロールは各ピアが独自に行い、インデクサーはRWI(Reverse Word Index)を作成する。作成されたインデックスの一部はDHT(分散ハッシュテーブル、Distributed Hash Table)として他のピアに分配される。
検索は自分のピアの端末からP2Pネットワーク上にある他のピアにリクエストを送信することにより行うことができる。
分散型検索エンジンの例としてはYaCyがある。YaCyは「人民による人民のためのウェブ検索」を標榜し、分散型であることにより検閲を防ぐことができるとしている。
ひとつの検索ワードを複数の検索エンジンで検索することをメタ検索という（横断検索エンジンと呼ぶこともある）。
詳細は「メタ検索エンジン」を参照のこと。
与えられた文書群から、検索式（キーワードなど）による全文検索機能を提供するソフトウェア、システムの総称で、ウェブサーバに組み込んで利用されることが多い。スタンドアローン環境で用いられる個人用途のものもあり、そういったものは特に「デスクトップ検索」と呼ばれている。
検索エンジンのはしりは1994年にスタンフォード大学のジェリー・ヤンとデビッド・ファイロが開発したYahoo!である。Yahoo!はディレクトリ型の検索エンジンで人手でウェブ上のページを収集して体系的に整理したものインターネットの普及に大きな役割を果たした。
その後、ウェブ上の情報を自動的に探索して情報を索引として整理するロボットまたはクローラと呼ばれるプログラムが開発された。ロボット型検索エンジンの中でもラリー・ペイジとセルゲイ・ブリンが開発したGoogle検索は検索結果のランキングと高速検索に優れていたため検索エンジンのトップに躍り出た。
2009年にはマイクロソフトが新たな検索エンジンとしてBingを発表した。
日本のインターネット普及初期から存在した検索エンジンには以下のようなものがある。黎明期には、豊橋技術科学大学の学生が作成したYahhoや、東京大学の学生が作成したODiN、早稲田大学の学生が作成した千里眼など、個人の学生が作成したものが商用に対して先行していた（いずれも1995年に作成、日本電信電話株式会社のNTT DIRCECTORY、サイバースペースジャパン（現・ウェブインパクト）のCSJインデックスは1994年に作成）。これらは単に実験用に公開されていただけでなく、多くの人に用いられていたものであり、黎明期のユーザにとっては知名度、実用度ともに高いものであった。またMondouなどのように研究室（京都大学）で作成したものもあった。
1995年12月にソフトバンクがアメリカ合衆国Yahoo!株を一部買い取り、翌年4月から日本版にローカライズしたYahoo! JAPANをサービス開始した。同年7月の展示会Interopでは机2つぶん並べる程度の小規模ブースで出展する程度の力の入れ具合で、ソフトバンクの一部署として開始する程度だったものが、もともとの米国Yahoo!の知名度、90年代後半のインターネット利用者人口の増加、ディレクトリ型だけだった検索をロボット型も追加、サイト登録した一部のウェブサイトの紹介をするYahoo! Internet Guide（ソフトバンククリエイティブ出版）との連携、日本Yahoo!株高騰のニュースでインターネットを利用しない人にも名前が知れ渡るなど、様々なプラス要因と経営戦略が見事に当たり、検索サイト首位の座を固めた。そして、検索サイトの集客力を武器にニュース、オークションなど、検索サービス以外のサービスを含めたポータルサイトとしての独走を始めた。
1997年頃から、WWW（World Wide Web）の爆発的な拡大に伴って、ディレクトリ型のみであったYahoo!のウェブディレクトリの陳腐化が急速に進んだ。この頃、infoseekやgooに代表されるロボット型検索エンジンが人気を集め始め、Yahoo! JAPANはロボット型検索エンジンにgooを採用するなど、群雄割拠の時代になった。
Googleが1998年に稼動させたGoogle検索は、従来の検索エンジンがポータルサイト化へと進む流れに逆行し、独創的な検索技術に特化し、バナー広告等を排除したシンプルな画面と2000年にYahoo!のロボット型検索エンジンに採用されたことにより、急速に人気を集めた。いつしかウェブページ検索の世界シェアのトップに躍り出たとされている。また日本においても、GoogleやYahoo!などの検索エンジンを利用すること＝「ググる」というネットスラングが生まれた。この状況に危機感を募らせたYahoo!は、2004年にロボット型検索エンジンを独自技術Yahoo!Search Technology (YST)（Yahoo!が買収したInktomiとAltaVista、Overture等の技術を統合した）に切り替えた。同年、GoogleやYahoo!のエンジンに匹敵すると言われるTeomaを利用した検索エンジン、Ask Jeeves（現・Ask.com）が「Ask.jp」として、2005年、オーストラリアで誕生したMooterが日本に進出し、検索サービスを開始した。
検索という行為が一般化するにつれて、各種目的別に多様化した検索エンジンが現れるようになった。ブログの情報に特化した検索TechnoratiやblogWatcher、商品情報の検索に特化した商品検索サイト、サイトの見た目で検索するMARSFLAG、音楽検索、動画検索、ファイル検索、アップローダ検索ほか、次々と新しい検索エンジンが生まれている。
また、検索エンジンでは判断できない抽象的な条件などでの検索を人手に求めた、OKWaveや人力検索はてななどの「人力検索」「ナレッジコミュニティ」と呼ばれるサービスも登場した。
近年ではパソコンだけでなく携帯電話や携帯型ゲーム機からもウェブサイトが検索される傾向が高くなり、GoogleやYahoo!をはじめとする携帯向けのモバイル検索サイトが登場し活気がでている。
ソフトバンク・Yahoo! JAPANがボーダフォンを買収し、KDDIがGoogleと提携するなど、携帯電話の分野で検索エンジンの戦いが激化してきている。モバイル検索の分野は長らく公式サイトと呼ばれる世界がユーザーの囲い込みを行っていたため、脚光を浴びることが少なかった。
Googleなどのウェブ検索エンジンでは、データベースの検索結果など多くの動的ページが検索対象になっていない。このような動的ページは「深層ウェブ」「見えないウェブ」「隠されたウェブ」などと呼ばれている。静的ページの500倍の量が存在し、多くは無料だといわれる。深層ウェブは、一般の検索エンジンなどからデータベースなどを見つけ出すか、直接アクセスした上で、それぞれの検索機能から再度検索しなければならない。
ロボット型検索エンジンは、その原理上インターネット上のコンテンツを複製の上で、検索を目的とした蓄積に適した形態で保存する他、場合によってはキャッシュとして提供できるような形態でも保存する場合がある。著作権をたてに、ウェブサイトの閲覧利用規約等と称して、一切のいかなる複製も禁ずるとするサイト等があり、どういったものかと古くより話題になっていた。
また、2006年11月には、日本の知的財産戦略本部コンテンツ専門調査会第3回企画WGにおいて、検索エンジンに関して「著作権法上、複製、編集には権利者の許諾が必要であり、Yahoo!、Googleなど大手検索システムのサーバーは海外に置かれているのが現状。」と報告され、これをうけて、2010年1月の改正で複製が合法とされた。
このことを拡大解釈したのか、あたかも著作権法のために、日本ではGoogleのような企業が育たなかったであるとか、日本におけるネット検索を妨げたのは著作権法である、といった論が巷に見られるが（フェアユース規定がない等の点は従来より指摘されてはいるが）、このWG報告以前に、著作権法によりネット検索の事業が妨げられた、というような話はない（ネット検索エンジンの勃興は2006年より更に遡ること10年、1990年代中盤で、日本で「千里眼」、米国で「AltaVista」等、ほぼ同時に同様の試みが始まっていた（前述のように、著作権を盾に公開している情報の利用を拒否する主張をする者が居たことは確かだが、判例などの普通に考える所の法的根拠がその時点で存在していたわけがない）。Googleが十分に成長し、日本語サービスを開始したのですら前世紀である）。
なお、この場合の"キャッシュ"とは、検索エンジンの内部使用のための複製や要約（スニペット）作成のための複製であ
2006年頃から日本ではURL（アドレス）を表示せず、社名や商品名などの検索キーワードを表示し、検索エンジンで検索させるように仕向けるテレビコマーシャルなどの広告表現が急増している。大抵はキーワードが書かれた状態の検索フォームとボタンを表示し、マウスクリックを促す演出がなされている。このような変化が生じた理由は不明であるが、各メディアの広告掲載基準の変更や、コマーシャルでURLを表示するのに比べてアクセス数を獲得しやすいことが増加の要因である。しかし検索結果に企業にとって不都合な情報が現れる場合があるため、グーグル八分のような検索結果の操作が行われるケースも考えられる。
現在、主流となっている広告手法として、ユーザーの検索結果後に広告を露出させる検索連動型広告と、サイトの中を分析し、そのサイトに合った広告を配信するコンテンツ連動型広告が主流で、オーバーチュアではインスタレットマッチという行動ターゲティング型の広告を現在推し進めている。
英語圏でも2013年ごろから「#wikipedia」のような番号記号を使った広告活動をおこなっている。
いわゆる「使用言語からみたインターネット人口の割合」はInternet Archiveを用いてEuro MarketingとGlobal Reachから過去の月次資料を整理すると次のような推移を辿っている。
1995年以前のInternet Societyによればインターネットで用いられている言語のうち英語が占める割合は85%とされていたが、その後のITの進歩や各国のインターネットの普及により多言語化が進み、上表に見られるように2000年の年末には英語と非英語の言語人口が逆転し、その傾向は継続している。
2005年2月2日の時点で、WWW検索エンジンの代表格であるGoogleでは80億を越す8,058,044,651ウェブページが登録されている。検索エンジンの利用者はそれら80億を越すウェブページから求める情報を容易に引き出せると思い込みがちであるが、例えば日本語入力のできないコンピュータなどの端末を用いて日本語サイトを検索することは容易ではない。同様に非英語圏の言語間の検索は中間に翻訳エンジンを介さないと検索作業は難しい。
インターネットの多言語化が今後も増加すると仮定した場合、言語間の壁をどのように乗り越えるかは今後の検索エンジンが抱える課題の一つとして挙げることができる。
検索エンジンが、利便性が高いが危険性も多く存在する事やその被害例について多くの参考文献や資料が存在する。その一部であるが、
検索エンジンの安全性に関する調査報告については、
ウイルス対策ソフトなどを提供するセキュリティベンダーの米マカフィーが、2007年6月4日「検索エンジンの安全性に関する調査報告」を発表し「検索エンジンは危険であり、検索エンジンにキーワードを入力して上位に現れるサイトの危険度を調べたら、広告として表示されるサイトは、そうでないサイトの2.4倍も危険率が高い」としている。
nikkeibp
また検索エンジンのキーワード検索結果には危険なリンクでいっぱいであり、
検索エンジンが自分を守ってくれると思ってはいけない。それどころか検索結果ランキングがサイトの安全性を反映していないことも多く、特に検索エンジン広告を訪れる場合、ユーザーは高いリスクにさらされると報告書ではこう警鐘を鳴らしている。
さらに、検索エンジンの提供サイトの危険度についての調査報告では、
同マカフィーが「検索エンジンの安全度調査」を発表し「最も危険な結果が多いのは米ヤフー」としている。
など。
