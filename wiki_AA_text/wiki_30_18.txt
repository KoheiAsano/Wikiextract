CPU
CPU（シーピーユー、）、中央処理装置（ちゅうおうしょりそうち）は、コンピュータにおける中心的な処理装置（プロセッサ）。
「CPU」と「プロセッサ」と「マイクロプロセッサ」という語は、ほぼ同義語として使われる場合も多いが、厳密には以下に述べるように若干の範囲の違いがある。大規模集積回路（LSI）の発達により1個ないしごく少数のチップに全機能が集積されたマイクロプロセッサが誕生する以前は、多数の（小規模）集積回路（さらにそれ以前はディスクリート）から成る巨大な電子回路がプロセッサであり、CPUであった。大型汎用機を指す「メインフレーム」という語は、もともとは多数の架（フレーム）から成る大型汎用機システムにおいてCPUの収まる主要部（メイン）、という所から来ている。また、パーソナルコンピュータ全体をシステムとして見た時、例えば電源部が制御用に内蔵するワンチップマイコン（マイクロコントローラ）は、システム全体として見た場合には「CPU」ではない。
CPUは記憶装置上にあるプログラムと呼ばれる命令列を順に読み込んで解釈・実行することで情報の加工を行う。CPUはコンピュータ内での演算を行なう中心であり、CPUは通常はバスと呼ばれる信号線を介して主記憶装置や入出力回路に接続され、何段階かの入出力回路を介して補助記憶装置や表示装置、通信装置などの周辺機器が接続され、データやプログラムなど情報のやりとりを行う。
このようなCPUを用いたプログラムによるコンピュータの逐次動作がほとんどのコンピュータの基本的な動作原理となっている。記憶装置上にプログラムを配置してから、プログラムを実行する方式をプログラム内蔵方式と言う。
現在のCPUは、部品としてはプロセッサの1種である。プロセッサの多くはマイクロチップとして実装されており、マイクロプロセッサやMPU (Micro Processing Unit) と呼ばれる。また、算術演算機能を強化し信号処理に特化したデジタルシグナルプロセッサ (DSP) や、メモリや周辺回路を搭載し組込み機器制御を目的としたマイクロコントローラ（マイコン）などの展開種も登場している。
専用の電子回路に比べると実行速度は遅いが、プログラムを変えるだけで多様な処理が行えることから、非常に多岐にわたる用途に使用でき、専用回路に比べ設計、修正が容易である。このため、CPUはおよそあらゆるシステムに内蔵され、現代の産業や生活の屋台骨を支える存在にまで普及している。現在最も普及しているCPUアーキテクチャとしてARMアーキテクチャが挙げられる。ARMアーキテクチャは1991年から数え2008年初頭に出荷個数が100億個を超えるなど、家電製品から工業製品、携帯機器などに至る多くのシステムに組み込まれ、機器制御を司っている。また、PCなど、現在の汎用コンピュータ製品における多くのシステムのメインCPUにx86アーキテクチャが用いられており、インテルのx86系CPU出荷数は1978年6月9日の8086発売から2003年までの25年で10億個を越えた。
CPUの構造は1949年に世界最初のストアードプログラム方式コンピュータであるEDSACが発表された時点で、すでに基本的原理が確立している。CPUの発達は、プロセス技術の微細化による高速化、パイプライン並列化（命令パイプライン、演算パイプライン）、命令並列化（スーパースケーラ、VLIW）、データ並列化（SIMD演算）、CPUコア並列化、スレッド並列化（同時マルチスレッディング）などに支えられている。
CPUは、全体を制御する制御装置、演算装置、データを一時記憶するレジスタ、メモリなどの記憶装置とのインタフェース、周辺機器との入出力装置とのインタフェース、などから構成される。
その他 浮動小数点演算を行うFPU（浮動小数点演算ユニット）、レジスタより多くの情報を一時記憶するキャッシュメモリ、DMAコントローラ、タイマー、シリアルインターフェースなどの機能をCPUと同一IC内にもつものもある。また、メモリから読み込んだ命令語を内部的なオペレーションに置き換える変換部を持つものもある。
クロック同期型のCPUは、クロック信号によって規則正しいタイミングで各部の動作を統制されている。
同じCPUであればクロック周波数が高い方が高速に動作し、一定時間に多くのことを処理できる。
1クロックで処理できる内容はCPUの設計により異なり、複数クロックで1つの機械語命令を実行するものから、1クロックで複数の命令を同時に実行できるものまである。クロック周波数が1GHzのCPUは、基本回路が1秒間に10億回の動作をする。
多くのCPUでは、大まかに言って制御装置が命令の解釈とプログラムの制御の流れを制御し、演算装置が演算を実行する。
高性能なCPUや、非ノイマン型のCPUや、画像処理向けのCPUは、同時に複数の命令を実行できるように複数の実行部を同一IC内に持っているものがある。
ノイマン型CPUの基本的な動作は、その実装に関わらずプログラムと呼ばれる命令列を順番に実行することである。
プログラムは数値列として何らかのメモリに格納されている。CPUでは、フェッチ、デコード、実行という3つのステップがほぼ必ず存在する。
最初の段階であるフェッチとは、実行すべき命令（ある数値または数値の並び）をプログラムの置かれたメモリから取り出すことである。メモリ上の実行すべき命令の位置はプログラムカウンタで指定される。プログラムカウンタはCPUが現在見ているプログラム上の位置を示しているとも言える。命令フェッチに使用されると、プログラムカウンタはフェッチしたぶんだけ増加させられる。
CPUがメモリからフェッチした命令によってCPUの次にすべきことが決定される。デコードでは、命令をCPUにとって意味のある形式に分割する。命令を表す数値をどう分割するかは、予めそのCPUの命令セットで決定される。命令の一部の数値は命令コードと呼ばれ、実行すべき処理を指定する。その他の部分はオペランドと呼ばれ、その命令で使用する情報を示している。たとえば加算命令のオペランドは加算すべき数値を示している。オペランドには数値そのものが書かれていたり、数値のある場所（メモリのアドレスかレジスタの番号）が書かれている。古い設計では、デコーダ（デコードを行う部分）は変更不可能なハードウェア部品だった。しかし、より複雑で抽象的なCPUや命令セットではマイクロプログラム方式がしばしば使われ、命令を様々な信号に変換するのを助けている。このマイクロプログラムは書き換え可能な場合があり、製造後でも命令デコード方法を変更することができる。
フェッチとデコードの次は、実行ステップが行われる。このステップでは、CPUの多くの部分が接続され（たとえばマルチプレクサを切り替えるなどして）指定された操作を実行する。たとえば、加算を要求されている場合、加算器が所定の入力と接続され、出力と接続される。入力は加算すべき数値を提供し、出力には加算結果が格納される。加算結果が大きすぎてそのCPUに扱えない場合、算術オーバーフローフラグをフラグレジスタ（ステータスレジスタ）にセットする（RISCではフラグレジスタが存在しない場合もある）。入力や出力にはいろいろなものが使用される。演算結果が一時的かあるいはすぐに利用される場合にはレジスタと呼ばれる高速で小さなメモリ領域に格納される。メモリも入力や出力に使われる。レジスタ以外のメモリは低速だが、コスト的には一般的なメモリの方が安価であり大量のデータを格納できるため、コンピュータには必須である。
いくつかの命令はプログラムカウンタを操作する。それらは一般にジャンプ命令と呼ばれ、ループを構成したり、条件分岐をしたり、サブルーチンを実現するのに使われる。また、多くの命令はフラグレジスタを変化させる。それらのフラグはプログラムの動作に影響を与える。たとえば比較命令は二つの値を比較してフラグレジスタにその大小を示す値をセットする。そして、その値を使用してその後の処理の流れを決定するのである。
命令を実行後、同じ流れが繰り返されて次の命令をプログラムカウンタにしたがってフェッチする。もっと複雑なCPUでは、複数の命令をフェッチし、デコードし、同時に実行することもできる。しかし、基本的にどんなCPUでもやっていることはここで説明した流れと同じである。
現代のCPUのような装置が出てくる以前、ENIACのような計算機は、実行する処理の内容を変える度に物理的に配線を変更していた。このような機械では、プログラムを変更するために物理的に再構成する必要がある（たとえばENIACなどではパッチパネルが使われた）ことから「プログラム固定計算機」と呼ばれることがある（なお、ENIACは非常に限られた機能と性能になるが、ある程度はプログラム内蔵方式的な動作もできた）。
CPUは、一般にソフトウェア（プログラム）を実行する装置として定義されるため、CPUと呼べる最初の装置はプログラム内蔵方式のコンピュータからである。プログラム内蔵方式の考え方は、ENIACの設計時にすでに存在していたが、マシンの完成を早期に可能とするため、ENIACの初期段階で採用されなかった。ENIACが完成する以前の1945年6月30日、数学者のジョン・フォン・ノイマンの名で、"First Draft of a Report on the EDVAC" という報告書が公開・配布された。この中で、プログラム内蔵方式のコンピュータの設計について概説されている（アイディアの元はENIACのプロジェクト中に検討されたもので、ノイマンは助言役として加わり、報告書の執筆者はノイマンである。報告書の著者がノイマンだけとされたことやアイディアを誰の功績とみるかについては諸説ある）。この報告書はEDSACなどに影響を与えた。EDVACは1949年8月に一応の完成を見、アバディーンに移された（モークリーとエッカートの離脱（理由については諸説）などがありごたついた。運用に入ったのは1951年）。EDVACは様々な命令の集まりを実行するよう設計されていた。命令を組み合わせることで実用的なプログラムを構成し、EDVACで動作させることができたのである。殊にEDVACではプログラムは高速なメモリに格納されており、物理的に配線を変更することで指定されるものではない点が重要である。ノイマン型の設計では、EDVACで動作させるプログラムを変更するにはメモリを書き換えればよかったのである（ノイマン型はプログラム内蔵だけでなく、プログラムがデータとして書き換え可能である点まで含む点に注意）。
結果としてノイマン型で先に完成したのは、EDSAC(1949年)やManchester Mark Iの試作機 "Baby" (1948年)である。EDVACは先に設計が始まっているが、設計者間のごたごたがあって完成が遅れた。また、アイデアレベルではZuse Z3を1941年に開発しているコンラッド・ツーゼもそれ以前にプログラム内蔵方式（書き換えでない点に注意）を考案していた（1936年に特許申請しているが、アメリカに出願した際にチャールズ・バベッジの解析機関との類似を指摘され、特許は成立していない。ツーゼはこのときまでバベッジの業績を知らなかったと思われる。なおZ3は1998年にチューリング完全であったことが示されている）。データとプログラムを同じ記憶装置に格納するかどうかという点が異なる方式として、ハーバード・アーキテクチャがある。これはEDVAC以前に完成したHarvard Mark Iに由来する。同機ではさん孔テープにプログラムを格納した。ノイマン型とハーバード型の大きな違いは、後者が命令とデータの格納場所と扱いを完全に分離していることであり、前者はどちらも同じ記憶領域に格納する。汎用CPUは基本的にノイマン型であるが、ハーバード・アーキテクチャも部分的に採用されている（キャッシュメモリなど）。
デジタル機器としてのCPUは、状態を変更したり表現したりするために、何らかのスイッチを必要とする。電気機械式から電子式への移行期には、リレーや真空管がスイッチとして使われた。これらは、従来の完全な機械式よりも高速にスイッチを切り替えられたが、チャタリングをはじめ、コイル（インダクタ）によって発生する高電圧などの問題があった。一方、真空管はチャタリングは起こさないが、機能するには熱が必要であり、劣化により動作中にカソードの電子放射能力が減退（エミッション減退）して動作不能になってしまう。真空管が劣化・故障したら、故障した部位を特定して交換しなければならない。したがって、初期の電子計算機は高速化は実現したものの、電気機械式計算機よりも信頼性が低かった。EDVACのような真空管計算機は故障と故障の間の平均時間（MTBF = Mean Time Between Failure）は約 8 時間であったが、Harvard Mark Iのようなリレー式計算機はほとんど故障しなかった。しかし、信頼性よりも性能が重視され、真空管式計算機が主流となっていった。当時の同期式CPUのクロック周波数は現在のCPUに比較すると非常に遅く、100kHz〜4MHz程度であった。これは、当時の論理素子（真空管）のスイッチング速度によって限界が定められていたのである。
CPUの設計と複雑さの進化は、さらに小型で信頼性の高い電子部品を使うことで達成された。その最初の進化は、新たに発明され急激に性能の向上したトランジスタの利用である。これによって、1950年代から1960年代には、かさばって信頼性の低い真空管やリレーは使われなくなり、トランジスタ製CPUが主流となった。この改善によってさらに複雑で信頼性のあるCPUを一枚から数枚のプリント基板で構成できるようになったのである。
1964年、IBMが発表したSystem/360アーキテクチャは、いろいろな性能と大きさのコンピュータとして実装され、それらのシリーズではプログラムを変更することなく動作させることができた。当時、たとえ同じメーカーであっても、サイズの違うコンピュータは互換性がないのが普通だったのである。この改善を成し遂げるため、IBMはマイクロプログラム方式を採用した。これは現在のCPUでも広く使われている手法である。System/360は大変な成功を収め、その後数十年間メインフレーム市場を支配し続け、現在のz/Architectureに至っている。
同じ1964年、DECも、「PDP-8」という後世に影響を与えたミニコンピュータを、科学分野や研究分野に向けてリリースした。DECは、後にさらに広く使われることとなる「PDP-11シリーズ」を発表したが、このシリーズは、後に集積回路（IC）が使えるようになると、それを使ったバージョンも製造されている。トランジスタを使ったCPUでは、新たな設計上の工夫をする余裕が生じ、SIMDやベクトル計算機と呼ばれるものが出現した。そのような初期の実験的設計は、後にクレイ社の製造したスーパーコンピュータのベースとなっている。
トランジスタを使ったコンピュータは、それ以前のものと比較していくつかの明確な利点があった。信頼性向上と消費電力低下はもちろん、トランジスタによるスイッチは切り替え時間が劇的に短縮されたため、CPUが高速化されたのである。トランジスタによるコンピュータでは動作周波数は数十MHzまで高速化された。
1970年代中頃に登場したマイクロプロセッサにより、CPUなどに使われるプロセッサは1チップの大規模集積回路（LSI IC）に集積されるようになった。当初は当時の微細化の限界から4ビットや8ビットのプロセッサであったが、1980年代には16ビットや32ビットで、それに加えプロセス保護などメインフレームに追いつくような機能を持つものや、周辺機能やメモリ等を集積した、いわゆるワンチップマイコンなども多数現れた。もうひとつの特色はMOS（特に1980年代後半からは、CMOS）であることである。原理上、消費電力は抑えられるが、当初は遅かったことから、電卓など消費電力が重要で速度が重要でない分野から広まったが、微細化が進めば進むほど静電容量が減り高速化できるという特長があり（デナード則）、動作周波数は当初の1MHz程度から、2010年代には数GHzまで上がっている。微細化は、より多くのゲートを載せることができる、ということでもあり、より複雑で高性能なプロセッサが作られるようにもなった（ただし近年は、性能向上以上に複雑化が進む傾向である（ポラックの法則））。微細化による集積度の向上の傾向はムーアの法則により定性的にモデル化されている。
複雑さ、大きさ、構造、一般的な形状はこの60年間で劇的に変化したが、高性能化の基本的なコンセプトは、だいたい1960年代に初めて現れた、というものが多い。たとえば、アウト・オブ・オーダー実行の方式であるscoreboardingもTomasuloのアルゴリズムも、最初に考案されたのは1960年代である。
21世紀現在のコンピュータは、ほぼ全てが「二値論理」方式であり、そのうちの全てではないがかなり多くが、二値論理に数の表現法として二進法をマッピングして演算などを行っているが、メインフレームや、電卓用に特別に設計されたマイコンなどには、広義の二進化十進表現に含まれるような方式でハードウェアによって直接に十進の計算を行う機能が強化されているものもある。1ビットが二進法の1桁である。ビット数を「ビット幅」などとも呼ぶ。
例えば、「ビット幅」や「データバス幅」が8ビットであるため8ビットCPUと呼ばれるCPUでは、主なレジスタ等の幅、あるいは、データバスの幅が8ビットである。8ビットでは、非負整数であれば二進法8桁で表せる範囲である「2の8乗 − 1」まで、つまり［0 〜 255］の範囲の整数が表現できる。また「アドレス幅」はCPUが直接にメモリを指し示す（アドレッシングする）範囲を制限する。例えば、アドレス幅が32ビットのCPUでは、そのCPUが直接指定できるアドレスの範囲は、2の32乗、つまり4,294,967,296個の異なる位置になる。System/360以降の多くの命令セットアーキテクチャ（ISA）では1バイトがアドレス付けの単位であるため（バイトアドレッシング）、4ギビバイトのメモリに、直接アクセスできる、ということになる。
これらはCPUのデータ幅やアドレス幅による単純な分類方法であり、実際のCPUではデータ信号線やアドレス指定方法に工夫することで、外部的に少ないデータバス幅や内部的に少ないアドレス幅でも効率的にメモリ・アクセスできるようにしているものがあるため、こういった分類は多少複雑になっている。
CPUを表現する場合のビット数の意味は以下の通りである。
1990年代以降は4ビットから64ビットまで多様なビット幅のCPUが製品化されている。高ビット幅のCPUは機能や性能が高い反面、高集積化や回路の複雑度から高価格で消費電力も大きく、低ビット幅のCPUは機能や性能が制限される代わりに安価で低消費電力であるなど特徴があり、状況に応じて使い分けられている。
1990年代後半から21世紀に入って、パソコン用CPUで一般化した（メインフレームではもっとずっと前から一般的だったが）、いくぶん新たなCPU高速化技術についてはマルチコアやVLIW、スーパースケーラなどがある。
CPUのビット数による用途の例を示す。
上記の分類に当てはまらないものとして、過去には、互いに結合し自由にビット長を増やす事ができる方式のCPUがあり、これはビットスライスプロセッサと呼ばれた。代表的な製品にAMDのAM2900シリーズなどが挙げられる。AM2901は、スイス連邦工科大学のLilithワークステーション等に使用されていた。またデータをバイト単位で扱うCPU（バイトマシン）の他、ワード単位で扱うCPU（ワードマシン）もある（日本電気のACOS-6など）。
最も基本的なCPUの低消費電力化技術は低電圧化であった。ロジック動作の信号線の電圧を低電圧化することは、低消費電力化につながると同時に信号を"Hi"と"Low"の間で高速に変更できるため動作速度の向上にも寄与した。
当初はリレーのような数十ボルトの動作電圧だったが、1980年代には5Vがデジタルコンピュータの標準的な動作電圧となり、1990年代には内部回路が3V程度の低電圧化を取り入れはじめ、外部との信号線でも同様の低電圧化が行なわれる頃には、CPUの内部ではさらに低い電圧が採用されるようになった。2009年現在では内部的には1V未満まで低電圧化が進められており、ノイズ耐性を考慮すればほぼ限界であると考えられている。
ほとんどのCPU（もっと言えばほとんどの順序回路）は同期式である。つまり、CPUは同期信号にしたがって動作するよう設計されている。この信号は「クロック信号」として知られていて、一定周期の矩形波の形であることが多い。電気信号の伝播速度からCPU内の信号経路の長さを考慮してクロック信号の周波数が決定される。この周波数は信号伝播の最悪ケースを考慮して決めなければならない。最悪ケースを考慮して周波数を決定すれば、CPU全体が波形のエッジ部分で動作するよう設計でき、CPUの設計を簡略化できると同時にトランジスタ数も減らすことができる。しかし、この設計手法の欠点としてCPU全体が最も遅い部分を待つように設計しなければならず、全体の高速化がその遅い部分によって制限される。この制限に対処するために命令パイプラインやスーパースケーラといった手法が採られてきた。
パイプラインだけでは同期式CPUの問題を全て解決することはできない。たとえば、クロック信号は他の電気信号の遅延に影響される。クロック周波数が高くなり、さらに複雑なCPUを動作させようとしたとき、全回路を同期させるのが困難になってきた。このため、新たな高性能CPUでは1つのクロック信号でCPU全体を同期するのではなく、いくつかのクロック信号で各部分を個別に同期させるようにしている。また、クロック周波数が高くなるにつれてCPUの発熱が大きな問題となってきた。クロック信号が"Hi"と"Low"を繰り返すことで多くのロジック回路が同様に"Hi"と"Low"を繰り返し、その回路が演算処理に使われていない時でもクロック信号が供給されている間は無駄に動作して発熱する。21世紀現在CPUに使用されている半導体回路では、信号電圧を"Hi"か"Low"に保持し続けるよりも"Hi"から"Low"や"Low"から"Hi"へ移る時に多くの電気エネルギーを消費する。このため、CPUに高速処理能力を求めるとクロック周波数が高くなり発熱も多くなって、さらに冷却する必要が生じる。
つまり、無駄にクロック信号を供給することを止めれば電力消費は抑えられ発熱も小さくなる。このように、演算処理に関与しない不要ブロックへのクロック信号の供給を止めると呼ばれる手法がある。
クロック信号で全体を一斉に動かすのをやめる、という手もある。非同期設計には独特の手法が必要で、同期設計と比較すると非常に難しい点があるが、消費電力と発熱の面で大きな利点がある。SRAMなどでは、クロックと関係なくアクセスできたほうが扱いに便利な場合もあり、非同期SRAMはごく一般的な製品である。また演算回路など、一般的なプロセッサ内部の一部に使われることもある。
一般に市販された製品としては、非同期設計を表に出したマイクロプロセッサはあまり一般的ではないが、研究室での試作といったレベルでは研究・試作はさかんに行われており、日本のものでは南谷らによるTITACなどが知られている。海外ではマンチェスター大によるARMベースのAMULETは（技術的ではない理由で中止にはなっているが）市販品に使用される予定があった。他にMIPS（R3000）ベースのMiniMIPSなどがある。
クロックを完全に無くするのではなく部分的に非同期化することで性能を高める工夫としては、非同期演算装置を使ってスーパースカラーのパイプラインを構成することで演算性能を上げようとした設計などがある。同期動作するCPUに比較して性能が向上するかどうかは定かではないが、少なくとも原理的には効果が期待できる。
ただし、21世紀以降に登場している最新の高性能CPUで使用されている半導体回路技術（プロセス技術）では、消費電力に対するリーク電流の比率が大きくなり、リーク電流はクロック信号の有無に関係が無いためにこの供給停止だけでは大きな効果がないようになっている。
このような高性能CPUでは、クロック信号の供給停止だけではなく、動作していないモジュール等への電源供給そのものを遮断すると呼ばれる技術も用いられている。これまでは、高性能化したCPUが消費する大電流をロジック回路に最適化された半導体回路技術（プロセス技術）で制御することは容易ではなかったが、ようやく解決されて2009年現在、市販製品に利用されつつある。
CPUを中心に拡張された電子部品にマイクロコントローラ（MCU）がある。
このMCUはCPUに加えてプログラム格納用を含む半導体メモリやGPIOとシリアルIO、DAC/ADCといった各種入出力機能にタイマーやDMACにクロック回路、必要に応じてDSPやフラッシュメモリなどの周辺回路を1つのパッケージに内蔵して、主に小型の組込み機器の制御に使用される。
CPU（Central Processing Unit）という用語は、コンピュータシステムにおいて中心的なプロセッサユニット、というような意味である（以前はプロセッサユニットとしてはCPUのみがある、というコンピュータがもっぱらであったので、ほぼ同義語という面があったが、近年はGPUなど他にも多数のプロセッサがある場合も一般的になっている）。マイクロプロセッサについては（メーカーによる用語の違いという面もあるが）「MPU」（Micro Processing Unit）という語もある。
日本においては、半導体由来であることから、古くからのトランジスタの呼称、あるいは一般の集積回路等と同様に「石」と呼ぶことがある。
また、比喩的表現（換喩法）でコンピュータ本体を指す事もある。
